---
title: "CS231n 学习总结"
tags: computer-vision deep-learning

article_header:
  type: cover
  image:
    src: /assets/images/post_images/cs231n/title.png
---

## 背景

寒假里草草看了CS231n，此为学习笔记以及相关概念的拓展。

<!--more-->
## 优化部分的相关算法

### 三种梯度下降

1. Gradient decent 
每次更新时使用全部样本。

2. Stochastic gradient descent 
每次更新时使用一个样本。

3. mini-batch gradient descent
每次更新时使用一批样本。

### Batch Normalization

大意是将每一层输入归一化，然后加入$\gamma$与$\beta$缩放与偏移后，喂给网络，有效解决梯度弥散和梯度爆炸的问题。

BN层一般是在激活层之后，全连接层之前。

![bn](/assets/images/post_images/cs231n/bn.png)

### SGD + Momentum (带动量的SGD)

常规SGD：

$$x_{t+1} = x_{t} - \alpha \nabla f(x_t)$$

SGD+Momentum：

$$ V_{t+1} = \rho v_t + \nabla f(x_t)$$
$$ x_{t+1} = x_t - \alpha v_{t+1} $$

其中$\rho$ 可以看做是摩擦系数，对下降的速度进行衰减。$\rho$ 一般为0.9或者0.99。

类似于小球滚下山时，即使经过极小值点或者鞍点，也会因为存在速度而越过这些点。

Nesterov Momentum:

$$ V_{t+1} = \rho v_t - \alpha \nabla f(x_t + \rho v_t) $$
$$ x_{t+1} = x_t + v_{t+1} $$

### AdaGrad

```python
grad_squared = 0
while True:
  dx = compute_gradient(x)
  grad_squared += dx * dx
  x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)
```

* AdaGrad 步长会越来越小。
* 当一个维度下降速度慢时，AdaGrad会加速，反之则会减速。

### RMSProp:AdaGrad的变种

```python
grad_squared = 0
while True:
  dx = compute_gradient(x)
  grad_squared += decay_rate* grade_squared + (1-decay_rate) * dx * dx
  x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)
```

### Adam 

``` python
first_moment = 0
second_moment = 0
while True:
  dx = compute_gradient(x)
  first_moment = beta1 * first_moment + (1-beta1) * dx
  second_moment = beta2 * second_moment + (1-beta2) * dx *dx
  x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7)
```

Adam实际上有点像将AdaGrad 与SDG+Momentum结合的结果。